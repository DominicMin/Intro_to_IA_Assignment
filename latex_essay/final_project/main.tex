\documentclass{xum_review}
%------------------------------------------------------------
% Packages
%------------------------------------------------------------
\usepackage{setspace}      % For line spacing
\usepackage{geometry}      % Page margins
\usepackage{fancyhdr}      % Header & footer
\usepackage{titlesec}      % Custom headings
\usepackage{csquotes}      % Quotation tools (optional)
\usepackage{listings}
\usepackage{xcolor} 
\usepackage{float}
\usepackage{caption}
% \usepackage{indentfirst}  % 注释掉以符合APA格式（首段不缩进）
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{adjustbox}
\usepackage{hyperref}
\usepackage{apacite}  % APA style with bibtex
\usepackage{tocloft}   % For customizing table of contents
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{lastpage}  % For getting total page count
\usepackage{fontspec}  % 添加fontspec包，XeLaTeX需要
% \usepackage{minted}  % 重新启用minted包


% XeLaTeX字体支持 - 添加Consolas字体
\setmonofont{Consolas}[Scale=0.9] % 设置等宽字体为Consolas，稍微缩放
% 或者如果想要更精细的控制：
% \newfontfamily\consolasfont{Consolas}[Scale=0.9]

% 代码格式 - 简约浅灰色风格，使用Consolas字体
\lstnewenvironment{python}[1][]
{
	% 添加语言标题 - 简约浅灰色窄条
	\vspace{0.5em}
	\noindent
	\colorbox[RGB]{232,232,232}{%
		\begin{minipage}{0.979\textwidth}
		\vspace{0.2em}
		\hspace{0.8em}
		\includegraphics[height=0.9em]{figure/py_logo.png}%
		\hspace{0.5em}%
		\color[RGB]{88,88,88}%
		\textbf{\textsf{Python}}%
		\vspace{0.2em}
		\end{minipage}
	}
	\vspace{-1.65em}
	\lstset{
		language=Python,
		basicstyle=\ttfamily\footnotesize, % XeLaTeX会自动使用Consolas
		numbers=left,
		numberstyle=\ttfamily\color[RGB]{130,130,130},
		numbersep=5pt,
		frame=none,
		framesep=0pt,
		xleftmargin=0pt,
		framexleftmargin=0pt,
		% backgroundcolor=\color[RGB]{255,255,255},
		backgroundcolor=\color[RGB]{245,245,245},
		breaklines=true,
		breakindent=10pt,
		keywordstyle=\color[RGB]{255,119,0},
		morekeywords={as, self},
		deletekeywords={print},
		keywordstyle=[2]\color[RGB]{144,0,144},
		morekeywords=[2]{print},
		stringstyle=\color[RGB]{0,170,0},
		commentstyle=\color[RGB]{221,0,0},
		showstringspaces=false,
		#1
	}
}{}

%JSON代码格式 - 简约浅灰色风格
\lstnewenvironment{json}[1][]
{
	% 添加JSON语言标题 - 简约浅灰色窄条
	\vspace{0.5em}
	\noindent
	\colorbox[RGB]{232,232,232}{%
		\begin{minipage}{0.979\textwidth}
		\vspace{0.2em}
		\hspace{0.8em}
		\includegraphics[height=0.9em]{figure/json_logo.png}%
		\hspace{0.5em}%
		\color[RGB]{88,88,88}%
		\textbf{\textsf{JSON}}%
		\vspace{0.2em}
		\end{minipage}
	}
	\vspace{-1.65em}
	\lstset{
		language=,
		basicstyle=\ttfamily\footnotesize,
		numbers=left,
		numberstyle=\ttfamily\color[RGB]{130,130,130},
		numbersep=5pt,
		frame=none,
		framesep=0pt,
		xleftmargin=0pt,
		framexleftmargin=0pt,
		% framexleftmargin=2pt,
		% backgroundcolor=\color[RGB]{255,255,255},
		backgroundcolor=\color[RGB]{245,245,245},
		breaklines=true,
		breakindent=10pt,
		% JSON特有的语法高亮
		stringstyle=\color[RGB]{0,170,0},      % 字符串为绿色
		keywordstyle=\color[RGB]{0,0,255},     % 关键字为蓝色
		% numberstyle=\color[RGB]{255,119,0},    % 数字为橙色
		commentstyle=\color[RGB]{221,0,0},     % 注释为红色（虽然JSON标准不支持注释）
		% 定义JSON的特殊字符高亮
		literate=
		{:}{{{\color[RGB]{255,119,0}{:}}}}{1}
		{,}{{{\color[RGB]{255,119,0}{,}}}}{1}
		{\{}{{{\color[RGB]{0,0,255}{\{}}}}{1}
		{\}}{{{\color[RGB]{0,0,255}{\}}}}}{1}
		{[}{{{\color[RGB]{0,0,255}{[}}}}{1}
		{]}{{{\color[RGB]{0,0,255}{]}}}}{1}
		{true}{{{\color[RGB]{144,0,144}{true}}}}{4}
		{false}{{{\color[RGB]{144,0,144}{false}}}}{5}
		{null}{{{\color[RGB]{144,0,144}{null}}}}{4},
		showstringspaces=false,
		morestring=[b]",
		morestring=[b]',
		#1
	}
}{}

%HTML代码格式 - 简约浅灰色风格
\lstnewenvironment{html}[1][]
{
	% 添加HTML语言标题 - 简约浅灰色窄条
	\vspace{0.5em}
	\noindent
	\colorbox[RGB]{232,232,232}{%
		\begin{minipage}{0.979\textwidth}
		\vspace{0.2em}
		\hspace{0.8em}%
		\includegraphics[height=1em]{figure/html_logo.png}%
		\hspace{0.5em}%
		\color[RGB]{88,88,88}%
		\textbf{\textsf{HTML}}%
		\vspace{0.2em}
		\end{minipage}
	}
	\vspace{-1.65em}
	\lstset{
		language=HTML,
		basicstyle=\ttfamily\footnotesize,
		numbers=left,
		numberstyle=\ttfamily\color[RGB]{130,130,130},
		numbersep=5pt,
		frame=none,
		framesep=0pt,
		xleftmargin=0pt,
		framexleftmargin=0pt,
		backgroundcolor=\color[RGB]{245,245,245},
		breaklines=true,
		breakindent=10pt,
		% HTML特有的语法高亮
		keywordstyle=\color[RGB]{0,0,255},        % HTML标签为蓝色
		stringstyle=\color[RGB]{0,170,0},         % 字符串为绿色
		commentstyle=\color[RGB]{221,0,0},        % 注释为红色
		identifierstyle=\color[RGB]{255,119,0},   % 属性为橙色
		showstringspaces=false,
		tabsize=2,
		#1
	}
}{}

%JavaScript代码格式 - 简约浅灰色风格
\lstnewenvironment{javascript}[1][]
{
	% 添加JavaScript语言标题 - 简约浅灰色窄条
	\vspace{0.5em}
	\noindent
	\colorbox[RGB]{232,232,232}{%
		\begin{minipage}{0.979\textwidth}
		\vspace{0.2em}
		\hspace{0.8em}
		\includegraphics[height=1em]{figure/js_logo.png}%
		\hspace{0.5em}%
		\color[RGB]{88,88,88}%
		\textbf{\textsf{JavaScript}}%
		\vspace{0.2em}
		\end{minipage}
	}
	\vspace{-1.65em}
	\lstset{
		language=Java,
		basicstyle=\ttfamily\footnotesize,
		numbers=left,
		numberstyle=\ttfamily\color[RGB]{130,130,130},
		numbersep=5pt,
		frame=none,
		framesep=0pt,
		xleftmargin=0pt,
		framexleftmargin=0pt,
		backgroundcolor=\color[RGB]{245,245,245},
		breaklines=true,
		breakindent=10pt,
		% JavaScript特有的语法高亮
		keywordstyle=\color[RGB]{144,0,144},      % 关键字为紫色
		keywordstyle=[2]\color[RGB]{255,119,0},   % 函数名为橙色
		morekeywords={var,let,const,function,return,if,else,for,while,do,switch,case,break,continue,try,catch,finally,throw,new,this,typeof,instanceof,in,of,async,await,class,extends,super,static,import,export,default,from,as},
		morekeywords=[2]{console,document,window,alert,prompt,confirm,getElementById,createElement,appendChild,addEventListener,fetch,Promise,JSON,Array,Object,String,Number,Boolean,Date,RegExp,Math,parseInt,parseFloat,isNaN,setTimeout,setInterval,clearTimeout,clearInterval},
		stringstyle=\color[RGB]{0,170,0},         % 字符串为绿色
		commentstyle=\color[RGB]{221,0,0},        % 注释为红色
		identifierstyle=\color[RGB]{0,0,0},       % 标识符为黑色
		showstringspaces=false,
		tabsize=2,
		morecomment=[l]{//},
		morecomment=[s]{/*}{*/},
		morestring=[b]",
		morestring=[b]',
		morestring=[b]`,
		#1
	}
}{}

% 通用代码环境 - 简约浅灰色风格
\newcommand{\codeheader}[1]{%
	\vspace{0.5em}
	\noindent
	\colorbox[RGB]{248,249,250}{%
		\begin{minipage}{\textwidth}
		\vspace{0.2em}
		\hspace{0.8em}
		\color[RGB]{88,88,88}%
		\textbf{\textsf{#1}}%
		\vspace{0.2em}
		\end{minipage}
	}
	\vspace{-0.1em}
}

\lstnewenvironment{codewithheader}[2][]
{
	\codeheader{#2}
	\lstset{
		basicstyle=\ttfamily\footnotesize,
		numbers=left,
		numberstyle=\tiny\color{gray},
		numbersep=8pt,
		frame=single,
		framerule=0.5pt,
		frameround=ffff,
		rulecolor=\color[RGB]{229,230,232},
		framesep=2pt,
		xleftmargin=0pt,
		framexleftmargin=2pt,
		backgroundcolor=\color[RGB]{255,255,255},
		breaklines=true,
		breakindent=10pt,
		keywordstyle=\color[RGB]{255,119,0},
		stringstyle=\color[RGB]{0,170,0},
		commentstyle=\color[RGB]{221,0,0},
		showstringspaces=false,
		#1
	}
}{}

% ===============================================
% VS Code 风格的 minted 环境（需要 --shell-escape）
% ===============================================

% 设置全局minted样式
% \usemintedstyle{monokai}

% % VS Code 风格的 Python 环境
% \newenvironment{vscode-python}[1][]
% {%
% 	% VS Code风格的深色标题栏
% 	\vspace{0.5em}%
% 	\noindent
% 	\colorbox[RGB]{30,30,30}{%
% 		\begin{minipage}{\textwidth}
% 		\vspace{0.2em}
% 		\hspace{0.8em}
% 		\includegraphics[height=0.9em]{figure/py_logo.png}%
% 		\hspace{0.5em}%
% 		\color[RGB]{204,204,204}%
% 		\textbf{\textsf{Python}}%
% 		\vspace{0.2em}
% 		\end{minipage}
% 	}%
% 	\vspace{-0.1em}%
% 	\begin{minted}[
% 		style=monokai,
% 		bgcolor=black,
% 		fontsize=\footnotesize,
% 		linenos,
% 		numbersep=8pt,
% 		xleftmargin=10pt,
% 		framesep=2mm,
% 		#1
% 	]{python}%
% }{%
% 	\end{minted}%
% }

% % VS Code 风格的 JSON 环境
% \newenvironment{vscode-json}[1][]
% {%
% 	% VS Code风格的深色标题栏
% 	\vspace{0.5em}%
% 	\noindent
% 	\colorbox[RGB]{30,30,30}{%
% 		\begin{minipage}{\textwidth}
% 		\vspace{0.2em}
% 		\hspace{0.8em}
% 		\includegraphics[height=0.9em]{figure/json_logo.png}%
% 		\hspace{0.5em}%
% 		\color[RGB]{204,204,204}%
% 		\textbf{\textsf{JSON}}%
% 		\vspace{0.2em}
% 		\end{minipage}
% 	}%
% 	\vspace{-0.1em}%
% 	\begin{minted}[
% 		style=monokai,
% 		bgcolor=black,
% 		fontsize=\footnotesize,
% 		linenos,
% 		numbersep=8pt,
% 		xleftmargin=10pt,
% 		framesep=2mm,
% 		#1
% 	]{json}%
% }{%
% 	\end{minted}%
% }

\begin{document}

\tableofcontents
\newpage
\setcounter{page}{1}

\section{Introduction}

Intelligent Q\&A systems have seen growing use in education, customer service, and online consultation. With advances in
NLP, machine learning, and large language models (LLMs), chatbots are now more capable of understanding complex queries
and managing multi-turn conversations.

In university settings, students often face practical questions related to course schedules, administrative procedures,
and campus facilities. Traditional methods of information access—such as browsing websites or asking peers—can be
time-consuming and inefficient. 

This project aims to design and implement a chatbot tailored to student inquiry scenarios. It accurately detects user
intent, recognizes key entities, and generates context-aware responses. By integrating multiple NLU components,
including intent classification, entity recognition, and sentiment analysis, the system enhances user experience,
streamlines information access, and improves overall information availability on campus.

\section{Project Methodology}

	\subsection{Research Design and Paradigm}

	This project adopts the Constructive Research / Design Science (DSR) research paradigm. The core objective is to design,
	build, and evaluate a novel AI system—a hybrid chatbot that integrates both retrieval-based and template-based
	features—to address real-world problems such as low information retrieval efficiency and lack of emotional interaction
	in campus settings\citep{yang2019hybridretrievalgenerationneuralconversation}.

	The overall research process follows an iterative development cycle:
	\begin{enumerate}
		\item	Requirement Analysis and Data Collection: Identify key information needs in campus scenarios and gather
		relevant raw data.
		\item	System Design and Implementation: Design and build a system architecture comprising a Natural Language
		Understanding (NLU) module, a response generation module, and an interactive front-end interface.
		\item	Model Training and Development: Train the core NLU model using synthetic data generated through
		LLM-based strategies.
		\item	System Integration and Evaluation: Integrate all modules into a working prototype and evaluate the
		system using both quantitative and qualitative methods.
		\item	Iterative Optimization: Refine the system by enriching training data and optimizing components based on
		evaluation feedback.
	\end{enumerate}

	This design-oriented research approach ensures that the outcomes are not only theoretically grounded but also
	practically valuable. The iterative methodology further enables rapid feedback incorporation and continuous system
	improvement in real-world usage scenarios.

	\subsection{System Architecture}

	The chatbot system consists of a front-end user interface and a back-end processing core. The back-end adopts a modular
	design, primarily comprising a Natural Language Understanding (NLU) module and a response generation module. The
	functions of each component will be described in detail in the following subsection\citep{mohammed2022chatbotarchitecture}.

	\subsection{Data Collection and Preparation}

	High-quality data is fundamental to the performance of natural language processing systems. In this study, data
	collection was conducted on two levels: (1) organizing raw information for knowledge base construction, and (2)
	generating training corpora for the NLU model. Both types of data underwent structured processing and manual
	verification to ensure their usability and reliability in system development.

	\subsubsection{Knowledge Base Data}
	We manually collected information on campus facilities, dining options, academic procedures, and more by consulting
	the student handbook, official public accounts, and other campus information sources, and structured it into a
	searchable \texttt{JSON} file.

	\subsubsection{Model Training Data}
	
	To train the NLU model, we adopted a synthetic data strategy. Specifically, we first designed a data generator
	(\texttt{data\_generator.ipynb}) that fills predefined intents and entities into prompt templates. Then, a locally deployed
	Ollama LLM is used to generate many feature-label pairs. Finally, all generated data undergo manual sampling,
	review, and correction to ensure quality and accuracy before being used for model training\citep{li2024datagenerationusinglarge}.

	\subsection{Evaluation Methods}

	To comprehensively evaluate the effectiveness of this project, we adopted a combination of quantitative and
	qualitative assessment methods\citep{0b014ee667a84c5985984bfd771595a3}.

	\subsubsection{Quantitative Evaluation}
	
	The evaluation primarily focuses on the performance of the NLU model. We used an independent test set and employed
	scikit-learn's \texttt{classification\_report} tool to automatically calculate a range of model evaluation metrics,
	including precision, recall, and F1-score—derived from the confusion matrix—to assess the model's performance across
	each class.

	\subsubsection{Qualitative Evaluation}

	The evaluation primarily targets the overall user experience and practicality of the chatbot system. We conducted
	multi-turn, open-ended conversations with the chatbot, with a particular focus on its ability to handle edge cases
	beyond the training data. This allowed us to assess the fluency of the dialogue, the accuracy of the responses, and
	the effectiveness of emotional interaction.

\section{Design of the Intelligent System}

	\subsection{Core Modules and Their Functions}

	\subsubsection{User Input Interface}

	This module is responsible for capturing user input and sending it to the back-end interpretation module.

	\subsubsection{Natural Language Understanding (NLU) Module}

	The Natural Language Understanding (NLU) module serves as the system's initial processing stage, responsible for
	transforming users' free-form text input into structured semantic information\citep{article}. It consists of the following three
	subsystems:

	\begin{itemize}
		\item{\textbf{Intent Classification}}\\
		This model aims to identify the user's query intent within a specific context. The task is formulated as a
		multi-class text classification problem, implemented using a Scikit-learn \texttt{pipeline} combining a \textit{TF-IDF} feature
		extractor and a \textit{logistic regression} classifier.
		\item{\textbf{Entity Recognition}}\\
		To further extract key information from user input (such as location, time, or objects), this task is modeled as a
		\textit{sequence labeling problem}. A custom Named Entity Recognition (NER) model is built using the \texttt{spaCy} framework,
		applying supervised learning to identify various entities in the text, thereby supporting knowledge retrieval.
		\item{\textbf{Sentiment Analysis}}\\
		To enhance the system's human-centered and emotional awareness capabilities, the \textit{VADER sentiment analysis} tool is
		integrated to classify the emotional tone and intensity of user input. The output includes positive, negative, and
		neutral sentiment categories, along with a compound score, which informs template selection and interaction tone
		adjustment.
	\end{itemize}

	\subsubsection{Response Generation Module}

	The response generation module is responsible for producing user-understandable and naturally styled text replies based
	on the intent, entities, and sentiment data output by the NLU module. It comprises the following two core components:

	\begin{itemize}
		\item{\textbf{Knowledge Base Retriever}}\\
		The system's knowledge is organized in a structured JSON format, covering common campus inquiry scenarios. The
		retriever receives keywords and labels from the NLU module and performs matching queries within the knowledge base
		to locate the most relevant information entries.
		\item{\textbf{Response Templating Engine}}\\
		A hierarchical response templating strategy is employed, with standardized language templates designed for different
		intent types. Based on the user's identified intent, extracted entities, and sentiment scores, the engine
		dynamically selects the appropriate template and fills in placeholders with information retrieved from the knowledge
		base, ultimately generating coherent and complete natural language responses.
	\end{itemize}

	\subsubsection{Interactive Front-end Interface}

	The front-end interface is built using a web framework and provides a chat-like, turn-based interaction experience.

	\subsection{Data Flow and Workflow}

	The system operates based on a single-turn interaction model following the sequence: \textit{Input → Interpretation → Generation
	→ Output}. For each user query, the system sequentially invokes its modules and returns a structured, readable response.
	The following describes a typical data flow during system operation.

	\subsubsection{Input Stage}

	System operation begins when the user submits a natural language query through the input interface, such as "What time
	does the library close?". Before entering the main processing pipeline, the input is first passed through a sentence
	segmentation step to enable sentence-by-sentence analysis for multi-sentence queries.

	\subsubsection{Natural Language Understanding Stage}

	Once the input text enters the Natural Language Understanding (NLU) module, it undergoes the following processing steps in sequence:
	\begin{itemize}
		\item	Intent Classification Each sentence is classified using an intent classifier built on TF-IDF features
	and a logistic regression model to identify its semantic purpose, such as "location inquiry," "time request," or
	"contact information request."
		\item	Named Entity Recognition (NER) A custom NER model trained with the spaCy framework extracts key entities
	from the input—such as "library," "academic office," or "17:30"—to support content generation in the response
	module.
		\item	Sentiment Analysis The VADER tool is used to perform basic sentiment polarity analysis, producing a
		positive, neutral, or negative tendency. This result can inform future system versions to support tone-adaptive
		responses.
	\end{itemize}

	After each clause is processed, the system organizes the output into a unified intermediate format. For example, given
	the input "What time does the library close?", the structured output is:

	\begin{json}
	{
		"query": "What time does the library close?",
		"intent": "ask_facility_time", 
		"entities": {"library": "LOCATION"},
		"sentiment": {"compound": 0.0}
	}
	\end{json}

	\subsubsection{Response Generation Stage}

	Based on the structured output, the system invokes the response generation module. The current version primarily relies
	on a rule-based matching mechanism: the system locates the appropriate response template according to the identified
	intent category and fills the predefined slots with the recognized entities to generate a semantically appropriate
	reply.

	\subsubsection{Output Stage}

	The final response text is processed by the output formatting module and then returned to the user interface for
	display, completing a full question-and-answer interaction cycle.

\section{Implementation}

	\subsection{System Architecture and Modules}

	\subsection{Prototype Screenshots}

	\subsection{System Execution and Testing}

\section{Discussion}

	\subsection{Summary of Overall Effectiveness}

	This system is designed to improve information access for university students and enhance the accessibility of
	campus-related content. During development, a modular architecture was adopted to facilitate system construction and
	deployment.

	Evaluation results indicate that the system performs reliably in intent classification, effectively covering most
	common campus-related queries. The sentiment analysis module also contributed to tone adjustment in responses,
	enhancing the naturalness and user-friendliness of interactions. While the current version does not yet support
	multi-turn dialogue or context modeling, it has demonstrated strong task completion and user experience in
	single-turn scenarios, meeting the intended design goals.


	\subsection{Advantages of Models and Systems}

	The system demonstrates several key strengths in both core modeling and overall architectural design. First, the
	intent and entity recognition models performed well on the test set, showing high accuracy and stability, which
	provides a reliable foundation for information retrieval. Additionally, the integration of the sentiment analysis
	module significantly enhanced the user interaction experience, making responses more human-like and
	context-sensitive.

	\subsection{Issues and challenges}

	During the development and deployment of the system, we also encountered several practical challenges and
	limitations. One of the major issues was the limited timeliness of the raw data, which made it difficult to keep up
	with real-time changes in campus operations and nearby businesses. In certain query scenarios, this could lead to
	outdated information, affecting the accuracy and credibility of the responses.

	\subsection{Directions for Improvement}

	Although the current system has demonstrated stable performance in single-turn question-answering scenarios, there
	are several areas for improvement in future iterations. First, incorporating a context-tracking mechanism would
	enable the system to understand multi-turn conversations, enhancing its ability to handle complex queries with
	greater coherence and continuity. Second, on the front-end side, further optimization of the interface design and
	response feedback mechanisms could improve usability and provide a smoother user experience.

\bibliography{ref}

\end{document}